{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import ttest_ind as ttest\n",
    "from scipy.stats import t as tpdf\n",
    "from math import *\n",
    "\n",
    "\n",
    "\n",
    "def generate_data(true_value:float, inter_cluster_SD:float, intra_cluster_SD:float, \\\n",
    "                  N_clusters:int, N_per_cluster:int):\n",
    "    \"\"\"\n",
    "    This function generates data. It randomly calculates the value for\n",
    "    experiments (the variation is set with SD). Data here has two levels of\n",
    "    hierachy: experiments per cluster and the number of cluster.\n",
    "    INPUT: true value of measurements, cluster-to-cluster variability,\n",
    "    experiment-to-experiment variability (inside a cluster), the number of\n",
    "    clusters, the number of experiments per cluster\n",
    "    OUTPUT: data - matrix of data (0 axis is experimental values per cluster;\n",
    "    1 axis is clusters)\n",
    "    \"\"\"\n",
    "    # generate matrix with clusters and experiments per cluster\n",
    "    data = true_value + inter_cluster_SD*np.random.randn(N_clusters, 1) + \\\n",
    "           intra_cluster_SD*np.random.randn(N_clusters,N_per_cluster)\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "def adj_ttest(N_per_cluster:int, N_clusters:int, inter_cluster_SD:float, \\\n",
    "              intra_cluster_SD:float, data_exp_pooled:list, \\\n",
    "              data_control_pooled:list):\n",
    "    N = N_per_cluster*N_clusters # the total number of experiments\n",
    "    # calculate intraclass correlation calculation:\n",
    "    ICC = inter_cluster_SD**2/(inter_cluster_SD**2 + intra_cluster_SD**2)\n",
    "\n",
    "    #item1 = (N_per_cluster - 1)*ICC\n",
    "    #item2 = (N - 2) - 2*item1\n",
    "    #item3 = (N - 2)*(1 + item1)\n",
    "    #c = sqrt(item2/item3) # correction factor for t-distribution\n",
    "    c=np.sqrt(((N-2)-2*(N_per_cluster-1)*ICC)/((N-2)*(1+(N_per_cluster-1)*ICC)))\n",
    "\n",
    "    #item4 = N-2*N_per_cluster\n",
    "    #item5 = (N-2)*(1-ICC)**2\n",
    "    #item6 = N_per_cluster*item4*ICC**2\n",
    "    #item7 = 2*item4*ICC*(1 - ICC)\n",
    "    #h = item2**2/(item5 + item6 + item7) # corrected degrees of freedom\n",
    "    h = ((N-2)-2*(N_per_cluster-1)*ICC)**2/((N-2)*(1-ICC)**2 + N_per_cluster*(N-2*N_per_cluster)*(ICC**2)+2*(N-2*N_per_cluster)*ICC*(1-ICC))\n",
    "\n",
    "    s=np.sqrt((N*data_exp_pooled.std()**2+N*data_control_pooled.std()**2)/(2*N-2))\n",
    "    #s = sqrt(((N-1)*np.std(data_exp_pooled)**2+(N-1)*np.std(data_control_pooled)**2)/(2*N-2)) # standard deviation of two datasets\n",
    "    t = abs(np.mean(data_exp_pooled) - np.mean(data_control_pooled))/(s*np.sqrt(1/N + 1/N)) # t-test\n",
    "    ta = c*t # corrected t-test\n",
    "    #p_value = 2*sum(tpdf.pdf(np.arange(ta,100,0.001),h)*0.001) # p-value = integral of t-distribution probability function\n",
    "    p_value = 2*(1-tpdf.cdf(ta, h))\n",
    "    #print('P-value based on t-distribution probability function is {:2.2f}'.format(p_value))\n",
    "    return ta, p_value\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def process_data(data_exp, data_control, N_per_cluster, N_clusters, \\\n",
    "                 inter_cluster_SD, intra_cluster_SD, data_method, ttest_method):\n",
    "    \"\"\"\n",
    "    This is the function to process data\n",
    "    There are several types of processing\n",
    "    By default it is use simple t-test on pooled data (ignore clustering)\n",
    "    INPUT: 1) the parameters for data generating\n",
    "            2) data_method = {‘pool’, ‘cluster’}, optional\n",
    "               choose the type of data to process furter\n",
    "               ( if 'pool', use the pooled data\n",
    "               elif 'cluster_means' use the means of clusters )\n",
    "            3) ttest_method = {'simple', 'adjusted'}, optional\n",
    "               choose what type of ttest to apply For more information read methods.md\n",
    "     \"\"\"\n",
    "\n",
    "    if data_method == 'pool': # use pooled data for processing\n",
    "        # pool the data into a list:\n",
    "        data_exp_pooled = data_exp.reshape(-1)\n",
    "        data_control_pooled = data_control.reshape(-1)\n",
    "        #print(data_exp, data_control)\n",
    "        if ttest_method == 'simple':\n",
    "            # use simple t-test\n",
    "            t, p_value = ttest(data_exp_pooled, data_control_pooled)\n",
    "        elif ttest_method == 'adjusted': # use adjusted t-test\n",
    "            t, p_value = adj_ttest(N_per_cluster, N_clusters, inter_cluster_SD, \\\n",
    "            intra_cluster_SD, data_exp_pooled, data_control_pooled)\n",
    "        else:\n",
    "            print('insert correct t-test method')\n",
    "    elif data_method == 'cluster':# use means of clusters for processing\n",
    "        data_exp_mean = data_exp.mean(axis=1)\n",
    "        data_control_mean = data_control.mean(axis=1)\n",
    "        if ttest_method == 'simple':\n",
    "            t, p_value = ttest(data_exp_mean, data_control_mean)\n",
    "        elif ttest_method == 'adjusted':\n",
    "            print('can\\'t do adjusted t-test. Need pooled data')\n",
    "            return\n",
    "        else:\n",
    "            print('insert correct t-test method')\n",
    "    return p_value\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def experiment(true_exp_value:float, true_control_value:float, \\\n",
    "               inter_cluster_SD:float, intra_cluster_SD:float, N_clusters:int, \\\n",
    "               N_per_cluster:int, data_method:str = 'pool', \\\n",
    "               ttest_method:str = 'simple'):\n",
    "    \"\"\"\n",
    "    This module generates data and asks another module for processing\n",
    "    There are several types of processing\n",
    "    By default it is use simple t-test on pooled data (ignore clustering)\n",
    "    For more information read documentation for process_data\n",
    "    INPUT:  1) the parameters for data generating\n",
    "            2) data_method = {‘pool’, ‘cluster’}, optional\n",
    "            3) ttest_method = {'simple', 'adjusted'}, optional\n",
    "    OUTPUT: the p-value of experiment\n",
    "    EXAMPLE_OF_USE: experiment(1, 1, 0.1, 0.2, 3, 5)\n",
    "                    experiment(1, 1, 0.1, 0.2, 3, 5, 'cluster', 'adjusted')\n",
    "    \"\"\"\n",
    "    # generate 2 matrices of data (control and experiment)\n",
    "    data_exp = generate_data(true_exp_value, inter_cluster_SD, intra_cluster_SD, \\\n",
    "                             N_clusters, N_per_cluster)\n",
    "    data_control = generate_data(true_control_value, inter_cluster_SD, \\\n",
    "                                 intra_cluster_SD, N_clusters, N_per_cluster)\n",
    "    # do the processing\n",
    "    p_value = process_data(data_exp, data_control, N_per_cluster, \\\n",
    "                                N_clusters, inter_cluster_SD, intra_cluster_SD, \\\n",
    "                                data_method, ttest_method)\n",
    "    return p_value\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def error_probability(NN:int, true_exp_value:float, true_control_value:float, \\\n",
    "                      inter_cluster_SD:float, intra_cluster_SD:float, N_clusters:int, \\\n",
    "                      N_per_cluster:int, data_method:str='pool', \\\n",
    "                      ttest_method:str='simple'):\n",
    "    \"\"\"\n",
    "    There are two types of errors: 1) False positive 2) False negative\n",
    "    what are the real values?\n",
    "    1) In case of unequal initial values we obtain error if p_value > 0.05\n",
    "       (this means that we agree on zero hypothesis) -> false positive error\n",
    "    2) If the real values are equal we obtain error if p_value < 0.05\n",
    "       (thus we reject zero hypothesis) -> false negative error\n",
    "    INPUT: NN - the number of experiments to conduct\n",
    "           and other parameters for experiment function\n",
    "    OUTPUT: the probability of error\n",
    "    \"\"\"\n",
    "    # sign s will easily help to make < reverse\n",
    "    if true_exp_value == true_control_value: s = 1\n",
    "    else: s = -1\n",
    "    # do NN experiments and see how many times we have an error\n",
    "    N_error = 0\n",
    "    for i in range(NN):\n",
    "        p_value = experiment(true_exp_value, true_control_value, inter_cluster_SD,\\\n",
    "                             intra_cluster_SD, N_clusters, N_per_cluster, \\\n",
    "                             data_method, ttest_method)\n",
    "        if s*p_value < s*0.05 :\n",
    "            N_error += 1\n",
    "    return N_error/NN\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def error_probability_heatmap(MAX_N_clusters:int, MAX_N_per_cluster:int, \\\n",
    "                              NN:int, true_exp_value:float, \\\n",
    "                              true_control_value:float, inter_cluster_SD:float, \\\n",
    "                              intra_cluster_SD:float, data_method:str='pool', \\\n",
    "                              ttest_method:str='simple'):\n",
    "    \"\"\"\n",
    "    Heatmap will show the error probability for an experimentator's choise\n",
    "    of number of clusters and number of measurements per cluster\n",
    "    INPUT: MAX_N_clusters - maximum number of clusters (vary from 1 to MAX)\n",
    "           MAX_N_per_cluster - maximum number of measurements per cluster\n",
    "           the parameters needed for error_probability function\n",
    "    OUTPUT: a matrix of probability with axis that correspond to the number\n",
    "            of clusters and the number od measurements per cluster\n",
    "    \"\"\"\n",
    "    CLUSTERS = np.array([i for i in range(2,MAX_N_clusters+1)])\n",
    "    PER_CLUSTER = np.array([i for i in range(2,MAX_N_per_cluster+1)])\n",
    "\n",
    "    probability = np.zeros((MAX_N_clusters-1, MAX_N_per_cluster-1))\n",
    "    for i, n_clusters in enumerate(CLUSTERS):\n",
    "        for j, n_per_cluster in enumerate(PER_CLUSTER):\n",
    "            probability[i, j] = error_probability(NN, true_exp_value, \\\n",
    "            true_control_value, inter_cluster_SD, intra_cluster_SD, n_clusters, \\\n",
    "            n_per_cluster, data_method, ttest_method)\n",
    "    return probability\n",
    "    #display_heatmap(probability, CLUSTERS, PER_CLUSTER)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def error_probability_ICC(NN:int, true_exp_value:float, \\\n",
    "                          true_control_value:float, inter_cluster_SD:float, \\\n",
    "                          intra_cluster_SD:float, N_clusters:int, \\\n",
    "                          N_per_cluster:int, data_method:str='pool', \\\n",
    "                          ttest_method:str='simple'):\n",
    "    \"\"\"\n",
    "    Let's calculate the probability of erroneus result in dependence of ICC\n",
    "    For this we make the intra_cluster_SD constant and vary inter_cluster_SD\n",
    "    Then call the function that calculates the probability of error for a\n",
    "    set of parameters\n",
    "    INPUT: all the parameters needed for error_probability counting\n",
    "    OUTPUT: a list of error probability for different ICC & ICC\n",
    "    \"\"\"\n",
    "\n",
    "    ICC = np.array([0.0, 0.01, 0.03, 0.07, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, \\\n",
    "                    0.4, 0.45, 0.5])\n",
    "    inter_cluster_SDs = np.sqrt(ICC*(intra_cluster_SD**2)/(1-ICC))\n",
    "\n",
    "    probability = np.zeros((len(ICC)))\n",
    "    for i, icc in enumerate(ICC):\n",
    "        probability[i] = error_probability(NN, true_exp_value, \\\n",
    "                                           true_control_value, inter_cluster_SDs[i],\\\n",
    "                                           intra_cluster_SD, N_clusters, \\\n",
    "                                           N_per_cluster, data_method,\\\n",
    "                                           ttest_method)\n",
    "    return probability, ICC\n",
    "    #display_graph(probability, ICC)\n",
    "    \n",
    "def error_probability_ICC(NN:int, true_exp_value:float, \\\n",
    "                        true_control_value:float,  \\\n",
    "                        intra_cluster_SD:float, N_clusters:int, \\\n",
    "                        N_per_cluster:int, ICC, \\\n",
    "                        data_method:str='pool', ttest_method:str='simple'):\n",
    "    \"\"\"\n",
    "    Let's calculate the probability of erroneus result in dependence of ICC\n",
    "    For this we make the intra_cluster_SD constant and vary inter_cluster_SD\n",
    "    Then call the function that calculates the probability of error for a\n",
    "    set of parameters\n",
    "\n",
    "    INPUT: all the parameters needed for error_probability counting\n",
    "\n",
    "    OUTPUT: a list of error probability for different ICC & ICC\n",
    "    \"\"\"\n",
    "\n",
    "    inter_cluster_SDs = np.sqrt(ICC*(intra_cluster_SD**2)/(1-ICC))\n",
    "\n",
    "    probability = np.zeros((len(ICC)))\n",
    "    for i in range(len(ICC)):\n",
    "        probability[i] = error_probability(NN, true_exp_value, \\\n",
    "                                            true_control_value, inter_cluster_SDs[i],\\\n",
    "                                            intra_cluster_SD, N_clusters, \\\n",
    "                                            N_per_cluster, data_method,\\\n",
    "                                            ttest_method)\n",
    "    return probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns #; sns.set_theme()\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "def display_data(data_exp, data_control):\n",
    "    \"\"\"\n",
    "    display data (all experiments and means per clusters)\n",
    "    INPUT: experimental data (matrix) & control data (matrix)\n",
    "    OUTPUT: None\n",
    "    \"\"\"\n",
    "    \n",
    "    #Determining the size of the input data\n",
    "    N_clusters_exp = len(data_exp)\n",
    "    N_clusters_control = len(data_control)\n",
    "    N_per_cluster_exp = len(data_exp[0])\n",
    "    N_per_cluster_control = len(data_control[0])\n",
    "    \n",
    "\n",
    "    #Calculate the average values for the clusters\n",
    "    data_exp_mean = data_exp.mean(axis=1)\n",
    "    data_control_mean = data_control.mean(axis=1)\n",
    "    \n",
    "    #Setting the parameters of the future chart\n",
    "    plt.rcParams[\"axes.edgecolor\"] = \"black\"\n",
    "    plt.rcParams[\"axes.linewidth\"] = 1\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    #Determining the colors of points of different clusters\n",
    "    color_exp = ['C{k}'.format(k=i) for i in range(N_clusters_exp)]\n",
    "    color_control = ['C{k}'.format(k=i + N_clusters_exp) for i in range(N_clusters_control)]\n",
    "\n",
    "\n",
    "    #Set a small offset of the points along the abscissa for clarity for the experimental data\n",
    "    arr_exp= np.ones((1,N_per_cluster_exp))+0.15/np.sqrt(N_clusters_exp)*np.random.randn(1,N_per_cluster_exp)\n",
    "    \n",
    "    #Plot these points of the corresponding colors for experimental data\n",
    "    for i in range(N_clusters_exp): \n",
    "            plt.plot(arr_exp[0], data_exp[i],'.',markersize=6,color=color_exp[i], alpha=0.2)\n",
    "    #Plotting crosses for mean values\n",
    "    plt.errorbar(np.ones(N_clusters_exp), data_exp_mean, xerr= data_exp.std(axis=1),yerr= data_exp.std(axis=1) ,ecolor=color_exp, elinewidth=3 , fmt='None')\n",
    "    \n",
    "    \n",
    "    #Similarly, everything is the same for control data\n",
    "    arr_control=2*np.ones((1,N_per_cluster_control))+0.15/np.sqrt(N_clusters_control)*np.random.randn(1,N_per_cluster_control)\n",
    "    for i in range(N_clusters_control):\n",
    "            plt.plot(arr_control[0], data_control[i],'.',markersize=6,color=color_control[i], alpha=0.2)\n",
    "    plt.errorbar(2*np.ones(N_clusters_control), data_control_mean, xerr= data_control.std(axis=1),yerr= data_control.std(axis=1) , ecolor=color_control, elinewidth=3, fmt='None' )\n",
    "\n",
    "    \n",
    "    #Setting chart parameters\n",
    "    ax.set_xlim(0,3) #Borders along the abscissa axis\n",
    "    ax.set_ylim(0,2) #Boundaries along the ordinate axis\n",
    "    ax.patch.set_visible(False)\n",
    "    plt.xticks([]) #Remove tick marks on the x-axis\n",
    "    plt.xlabel('exp                      control') #chart signature\n",
    "\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(0.2))#  Set the interval of large minor tick marks\n",
    "    ax.yaxis.set_minor_locator(ticker.MultipleLocator(0.1))#  Set the interval of small minor tick marks\n",
    "    ax.patch.set_visible(False) #Invisible background\n",
    "    ax.spines['right'].set_visible(False) #Invisible top line of boxing drawing\n",
    "    ax.spines['top'].set_visible(False)#Invisible right line drawing boxing\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def display_heatmap(probability, MAX_N_clusters, MAX_N_per_cluster, scaleMax=1):\n",
    "    \"\"\"\n",
    "    INPUT: probability is a matrix\n",
    "    OUTPUT: heatmap figure\n",
    "    \"\"\"\n",
    "    CLUSTERS = np.array([i for i in range(2,MAX_N_clusters+1)])\n",
    "    PER_CLUSTER = np.array([i for i in range(2,MAX_N_per_cluster+1)])\n",
    "    ax = sns.heatmap(probability.T, xticklabels = CLUSTERS, yticklabels = PER_CLUSTER, vmin = 0, vmax = scaleMax)\n",
    "    ax.invert_yaxis()\n",
    "    plt.xlabel('number of clusters')\n",
    "    plt.ylabel('number of measurements')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def display_graph(probability, ICC, label):\n",
    "    #Setting the parameters of the future chart\n",
    "    plt.rcParams[\"axes.edgecolor\"] = \"black\"\n",
    "    plt.rcParams[\"axes.linewidth\"] = 1\n",
    "    fig, ax = plt.subplots()\n",
    "    for i in range(len(probability[:,1])):\n",
    "        ax.scatter(ICC, probability[i,:], label=label[i])\n",
    "\n",
    "    ax.legend()\n",
    "    plt.xlabel('ICC')\n",
    "    # Set the interval of major tick marks\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(0.1))\n",
    "    # Set the interval of minor tick marks\n",
    "    ax.xaxis.set_minor_locator(ticker.MultipleLocator(0.05))\n",
    "\n",
    "    # Do the same with the divisions on the \"y\" axis:\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(0.1))\n",
    "    ax.yaxis.set_minor_locator(ticker.MultipleLocator(0.05))\n",
    "    \n",
    "    ax.patch.set_visible(False) #Invisible background\n",
    "    ax.spines['right'].set_visible(False) #Invisible top line of boxing drawing\n",
    "    ax.spines['top'].set_visible(False)#Invisible right line drawing boxing\n",
    "    ax.xaxis.set_ticks_position('bottom')\n",
    "    ax.yaxis.set_ticks_position('left')\n",
    "    plt.xlabel('ICC')\n",
    "    plt.ylabel('Probability of error')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframe(data_exp,data_control):\n",
    "    \n",
    "    \"\"\"\n",
    "    Create dataframe from experimental data (matrix) & control data (matrix)\n",
    "    INPUT: experimental data (matrix) & control data (matrix)\n",
    "    OUTPUT: dataframe based on experimental data and control data\n",
    "            dataframe['control or experiment'] - value from control data or experiment data\n",
    "            dataframe['Arbitrary unit'] - this value\n",
    "            dataframe['Ni'] - cluster number using end-to-end cluster numbering from experimental and control data\n",
    "    \"\"\"\n",
    "    #Checking whether it is represented by a two-dimensional matrix, or otherwise - a one-dimensional matrix.\n",
    "    #Depending on this, we set the size of the data. This is for an experimental data\n",
    "    if type(data_exp[0]) == np.ndarray or type(data_exp[0]) == list:\n",
    "        N_clusters_exp = len(data_exp)\n",
    "        N_per_cluster_exp = len(data_exp[0])\n",
    "        Ni_exp = [] \n",
    "        for Ni, cluster in enumerate(data_exp):\n",
    "            for _ in cluster:\n",
    "                Ni_exp.append(Ni)\n",
    "        Ni_exp = np.array(Ni_exp)\n",
    "    else: \n",
    "        N_clusters_exp = 1\n",
    "        N_per_cluster_exp = len(data_exp)\n",
    "        Ni_exp = np.array([i for i in range(len(data_exp))])\n",
    "\n",
    "    #Likewise for control data\n",
    "    if type(data_control[0]) == np.ndarray or type(data_control[0]) == list:\n",
    "        N_clusters_control = len(data_control)\n",
    "        N_per_cluster_control = len(data_control[0])\n",
    "        Ni_control = [] \n",
    "        for Ni, cluster in enumerate(data_control):\n",
    "            for _ in cluster:\n",
    "                Ni_control.append(Ni+1+max(Ni_exp))\n",
    "        Ni_control = np.array(Ni_control)\n",
    "    else: \n",
    "        N_clusters_control = 1\n",
    "        N_per_cluster_control = len(data_control)\n",
    "        Ni_control = np.array([(i)+1+max(Ni_exp)  for i in range(len(data_control))])\n",
    "\n",
    "    #Create a list with data belonging to a specific group\n",
    "    x_exp = np.array(['experiment' for i in range(len(pooled(data_exp)))])\n",
    "    x_control = np.array(['control' for i in range(len(pooled(data_control)))])\n",
    "\n",
    "    #create a dictionary corresponding to the future dataframe\n",
    "    d = {'control or experiment': np.concatenate((x_exp,x_control), axis=0),\n",
    "         'Arbitrary unit' : np.concatenate((pooled(data_exp), pooled(data_control)), axis=0),\n",
    "         'Ni': np.concatenate((Ni_exp,Ni_control), axis=0)}\n",
    "    #Create dataframe\n",
    "    df = pd.DataFrame(data=d)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_data_Superplot(data_exp,data_control):\n",
    "    \"\"\"\n",
    "    display data (all experiments and means per clusters) using Superplot\n",
    "    INPUT: experimental data (matrix) & control data (matrix)\n",
    "    OUTPUT: None\n",
    "    \"\"\"\n",
    "    #Сreate dataframe from our data\n",
    "    df = create_dataframe(data_exp,data_control)\n",
    "    df_mean = create_dataframe(means(data_exp),means(data_control))\n",
    "    \n",
    "    \n",
    "    #Plotting swarmplot for all points\n",
    "    ax = sns.swarmplot(x='control or experiment', y='Arbitrary unit', hue=\"Ni\", data=df, size=4,alpha=0.5, zorder=1, \\\n",
    "                  palette=sns.color_palette(\"Spectral\",n_colors=len(df_mean)))\n",
    "    \n",
    "    #Plotting swarmplot for cluster averages\n",
    "    ax = sns.swarmplot(x=\"control or experiment\", y=\"Arbitrary unit\", hue=\"Ni\", size=10, edgecolor=\"k\", linewidth=1, \\\n",
    "                       data=df_mean, alpha=1, zorder=2,\\\n",
    "                       palette=sns.color_palette(\"Spectral\",n_colors=len(df_mean)))\n",
    "    \n",
    "    #Build the standard deviation from the mean values for the clusters\n",
    "    ax.errorbar([0, 1], [means(data_exp).mean(), means(data_control).mean()],  xerr=[0.2, 0.2],\\\n",
    "                color='black', elinewidth=2, \\\n",
    "                linewidth=0, zorder=3,  capsize=0)\n",
    "    ax.errorbar([0, 1], [means(data_exp).mean(), means(data_control).mean()],  \\\n",
    "                yerr= [means(data_exp).std()/(np.sqrt(len(data_exp))), means(data_control).std()/(np.sqrt(len(data_control)))], color='black', elinewidth=2, \\\n",
    "                linewidth=0, zorder=3,  capsize=4)\n",
    "    \n",
    "    \n",
    "    ax.get_legend().remove()#Removing the legend\n",
    "    plt.xlim(-0.7,1.7)#Setting the limits on the abscissa axis\n",
    "    #plt.ylim(0,2)#Setting the limits on the ordinate axis\n",
    "    plt.xlabel('')#X-axis signature\n",
    "    ax.patch.set_visible(False) #Invisible background\n",
    "    ax.spines['right'].set_visible(False) #Invisible right line drawing boxing\n",
    "    ax.spines['top'].set_visible(False)#Invisible top line drawing boxing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze(data_exp, data_control, forecast_error=False, NN=1000, plus_number_cluster=1):\n",
    "    \"\"\"\n",
    "    Analyze your data and print value: Means, standart deviation and p value adjusted\n",
    "    Display data Superplot\n",
    "    Forecast probability of false negative error if your mean and SD are true\n",
    "    INPUT:  experimental data (matrix) & control data (matrix)\n",
    "            forecast_error : bool - Do the forecast probability of false negative error?\n",
    "            NN - number of trials to calculate the probability\n",
    "            plus_number_cluster - step of increasing the number of clusters in the forecast error\n",
    "    OUTPUT: None\n",
    "                    \n",
    "    \"\"\"\n",
    "    inter_cluster_SD, intra_cluster_SD = standard_deviation(data_exp, data_control)\n",
    "    \n",
    "    print('Mean experimetal = ', round(pooled(data_exp).mean(),3))\n",
    "    print('Mean control = ', round(pooled(data_control).mean(), 3))\n",
    "    print()\n",
    "    print('inter cluster SD =', round(inter_cluster_SD, 3))\n",
    "    print('intra cluster SD =', round(intra_cluster_SD, 3))\n",
    "    ICC = icc_calculator(data_exp, data_control)\n",
    "    print('ICC = ', round(ICC, 3))\n",
    "    \n",
    "    display_data_Superplot(data_exp, data_control)\n",
    "    print('\\n')\n",
    "    \n",
    "    \n",
    "    if scipy.stats.normaltest(pooled(data_exp))[1] < 0.05:\n",
    "        print('WARNING: The experimental data are not normally distributed (with a significance level of 0.05)', '\\n', 'Further results may be incorrect')\n",
    "    if scipy.stats.normaltest(pooled(data_control))[1] < 0.05:\n",
    "        print('WARNING: The control data are not normally distributed (with a significance level of 0.05)', '\\n', 'Further results may be incorrect')\n",
    "    \n",
    "    if len(data_exp) == len(data_control):\n",
    "        N_clusters = len(data_exp)\n",
    "    else:\n",
    "        print('WARNING N_clusters control and experiment are different')\n",
    "        N_clusters  =  len(data_exp) + len(data_control)\n",
    "        \n",
    "    N_per_cluster = 0\n",
    "    for data in data_exp:\n",
    "        N_per_cluster += len(data)\n",
    "    for data in data_control:\n",
    "        N_per_cluster += len(data)\n",
    "    N_per_cluster = N_per_cluster/(len(data_exp)+len(data_control))\n",
    "    \n",
    "    for data in data_exp:\n",
    "        if abs((len(data) - N_per_cluster)/ N_per_cluster) > 0.2:\n",
    "            print('WARNING: N per cluster : one value is 20% more than the average. Further results may give inaccurate results.')\n",
    "    for data in data_control:\n",
    "        if abs((len(data) - N_per_cluster)/ N_per_cluster) > 0.2:\n",
    "            print('WARNING: N per cluster one value is 20% more than the average. Further results may give inaccurate results.')\n",
    "        \n",
    "        \n",
    "        \n",
    "    p_value = adj_ttest(N_per_cluster=N_per_cluster, N_clusters = N_clusters,\\\n",
    "                        inter_cluster_SD = inter_cluster_SD, \\\n",
    "              intra_cluster_SD = intra_cluster_SD, data_exp_pooled = pooled(data_exp), \\\n",
    "              data_control_pooled = pooled(data_control))[1]\n",
    "    print('p value adjusted = ', round(p_value,3))\n",
    "    if p_value < 0.05:\n",
    "        print('Reject the null hypothesis of equality of means with a significance level of 0.05')\n",
    "    else:\n",
    "        print('There is no reason to reject the null hypothesis of equality of means with a significance level of 0.05')\n",
    "                                \n",
    "    print('\\n')\n",
    "    pb_err = error_probability(NN=NN, true_exp_value=pooled(data_exp).mean(), true_control_value=pooled(data_control).mean(), \\\n",
    "                          inter_cluster_SD=inter_cluster_SD, intra_cluster_SD=intra_cluster_SD,\\\n",
    "                            N_clusters=len(data_exp), \\\n",
    "                          N_per_cluster=len(data_exp[0]), data_method='pool', \\\n",
    "                          ttest_method='adjusted')\n",
    "    \n",
    "    print('Let\\'s assume that the measured parameters are true and simulate data based on them.')\n",
    "\n",
    "    print('So, the probability of false negative error is', pb_err)\n",
    "    \n",
    "    if forecast_error :\n",
    "        k = plus_number_cluster\n",
    "        while pb_err > 0.2:\n",
    "            pb_err = error_probability(NN=NN, true_exp_value=pooled(data_exp).mean(), true_control_value=pooled(data_control).mean(), \\\n",
    "                              inter_cluster_SD=inter_cluster_SD, intra_cluster_SD=intra_cluster_SD,\\\n",
    "                                N_clusters=(k + len(data_exp)), \\\n",
    "                              N_per_cluster=len(data_exp[0]), data_method='pool', \\\n",
    "                              ttest_method='adjusted')\n",
    "            print('If there were ', k + len(data_exp), ' clusters, the false negative error would be' , pb_err)\n",
    "            k += plus_number_cluster\n",
    "            \n",
    "    pb_err = error_probability(NN=NN, true_exp_value=(pooled(data_exp).mean()+pooled(data_control).mean())/2, \\\n",
    "                               true_control_value=(pooled(data_exp).mean()+pooled(data_control).mean())/2, \\\n",
    "                          inter_cluster_SD=inter_cluster_SD, intra_cluster_SD=intra_cluster_SD,\\\n",
    "                            N_clusters=len(data_exp), \\\n",
    "                          N_per_cluster=len(data_exp[0]), data_method='pool', \\\n",
    "                          ttest_method='adjusted')\n",
    "    \n",
    "    print('\\n')\n",
    "    print('Let\\'s assume that your SD are true and \\n mean_exp = mean_control = (mean_exp+ mean_control)/2 = ', \\\n",
    "          round((pooled(data_exp).mean()+pooled(data_control).mean())/2 ,3))\n",
    "\n",
    "    print('So, the probability of false positive error is', pb_err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_deviation(data_exp,data_control):\n",
    "    \"\"\"\n",
    "    Calculate the standard deviation inter cluster and intra cluster based on experimental data\n",
    "    INPUT: experimental data (matrix) & control data (matrix)\n",
    "    OUTPUT: inter cluster SD, intra cluster SD\n",
    "    \"\"\"\n",
    "    inter_cluster_SD = s_inter(data_exp, data_control)\n",
    "    \n",
    "    intra_cluster_SD = s_intra(data_exp, data_control)\n",
    "    \n",
    "    return inter_cluster_SD, intra_cluster_SD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def icc_calculator(data_exp,data_control):\n",
    "    \"\"\"\n",
    "    Calculate the ICC (intra-cluster correlation coefficient) based on experimental data\n",
    "    INPUT: experimental data (matrix) & control data (matrix)\n",
    "    OUTPUT: ICC (intra-cluster correlation coefficient)\n",
    "    \"\"\"\n",
    "    inter_cluster_SD, intra_cluster_SD = standard_deviation(data_exp, data_control)\n",
    "    ICC = inter_cluster_SD**2/(inter_cluster_SD**2 + intra_cluster_SD**2)\n",
    "    \n",
    "    return ICC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pooled(data):\n",
    "    \"\"\"\n",
    "    INPUT:  data (matrix)\n",
    "    OUTPUT: data reshape into (1,n) - line\n",
    "    \"\"\"\n",
    "    pooled = []\n",
    "    for x in data:\n",
    "        try :\n",
    "            for y in x:\n",
    "                pooled.append(y)\n",
    "        except:\n",
    "            pooled.append(x)\n",
    "\n",
    "    return np.array(pooled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def means(data):\n",
    "    \"\"\"\n",
    "    INPUT:  data (matrix)\n",
    "    OUTPUT: means data of clusters\n",
    "    \"\"\"\n",
    "    means = []\n",
    "    for x in data:\n",
    "        means.append(np.array(x).mean())\n",
    "    return np.array(means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def s_inter(data_exp, data_control):\n",
    "    \"\"\"\n",
    "    Calculate the standard deviation inter cluster based on experimental data\n",
    "    INPUT: experimental data (matrix) & control data (matrix)\n",
    "    OUTPUT: inter cluster SD\n",
    "    \"\"\"\n",
    "    s2 = 0\n",
    "    mean_exp = pooled(data_exp).mean()\n",
    "    mean_control = pooled(data_control).mean()\n",
    "    for x in means(data_exp):\n",
    "        s2 += (x - mean_exp)**2\n",
    "    for y in means(data_control):\n",
    "        s2 += (y  - mean_control)**2\n",
    "    s2 = s2 / (len(data_exp) + len(data_control) - 2)\n",
    "    return np.sqrt(s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def s_intra(data_exp, data_control):\n",
    "    \"\"\"\n",
    "    Calculate the standard deviation intra cluster based on experimental data\n",
    "    INPUT: experimental data (matrix) & control data (matrix)\n",
    "    OUTPUT:  intra cluster SD\n",
    "    \"\"\"\n",
    "    s2 = 0\n",
    "    for x in data_exp:\n",
    "        y = np.array(x)\n",
    "        mean = y.mean()\n",
    "        for i in y:\n",
    "            s2 += (i - mean)**2\n",
    "    for x in data_control:\n",
    "        y = np.array(x)\n",
    "        mean = y.mean()\n",
    "        for i in y:\n",
    "            s2 += (i - mean)**2\n",
    "    s2  = s2 / (len(pooled(data_exp)) + len(pooled(data_control)) - len(data_exp) - len(data_control))\n",
    "    return np.sqrt(s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file:str):\n",
    "    matrix = []\n",
    "    with open(file, 'r') as f:\n",
    "        m = [[float(num) for num in line.split()] for line in f]\n",
    "    matrix = np.array(m)\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file_csv(file:str):\n",
    "    matrix = []\n",
    "    with open(file, 'r') as f:\n",
    "        for line in f:\n",
    "            cluster = []\n",
    "            for num in line.split(';'):\n",
    "                try: \n",
    "                    x = float(num)\n",
    "                    cluster.append(x)\n",
    "                except:\n",
    "                    pass\n",
    "            matrix.append(cluster)\n",
    "        #m = [[float(num) for num in line.split(';')] for line in f]\n",
    "    matrix = np.array(matrix)\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
