{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import ttest_ind as ttest\n",
    "from scipy.stats import t as tpdf\n",
    "from math import *\n",
    "\n",
    "\n",
    "\n",
    "def generate_data(true_value:float, inter_cluster_SD:float, intra_cluster_SD:float, \\\n",
    "                  N_clusters:int, N_per_cluster:int):\n",
    "    \"\"\" This function generates data. It randomly calculates the value for\n",
    "    experiments (the variation is set with SD). Data here has two levels of\n",
    "    hierachy: experiments per cluster and the number of cluster.\n",
    "    INPUT: true value of measurements, cluster-to-cluster variability,\n",
    "    experiment-to-experiment variability (inside a cluster), the number of\n",
    "    clusters, the number of experiments per cluster\n",
    "    OUTPUT: data - matrix of data (0 axis is experimental values per cluster;\n",
    "    1 axis is clusters) \"\"\"\n",
    "    # generate matrix with clusters and experiments per cluster\n",
    "    data = true_value + inter_cluster_SD*np.random.randn(1,N_clusters) + \\\n",
    "           intra_cluster_SD*np.random.randn(N_per_cluster,N_clusters)\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "def adj_ttest(N_per_cluster:int, N_clusters:int, inter_cluster_SD:float, \\\n",
    "              intra_cluster_SD:float, data_exp_pooled:list, \\\n",
    "              data_control_pooled:list):\n",
    "    N = N_per_cluster*N_clusters # the total number of experiments\n",
    "    # calculate intraclass correlation calculation:\n",
    "    ICC = inter_cluster_SD**2/(inter_cluster_SD**2 + intra_cluster_SD**2);\n",
    "\n",
    "    #item1 = (N_per_cluster - 1)*ICC\n",
    "    #item2 = (N - 2) - 2*item1\n",
    "    #item3 = (N - 2)*(1 + item1)\n",
    "    #c = sqrt(item2/item3) # correction factor for t-distribution\n",
    "    c=np.sqrt(((N-2)-2*(N_per_cluster-1)*ICC)/((N-2)*(1+(N_per_cluster-1)*ICC)))\n",
    "\n",
    "    #item4 = N-2*N_per_cluster\n",
    "    #item5 = (N-2)*(1-ICC)**2\n",
    "    #item6 = N_per_cluster*item4*ICC**2\n",
    "    #item7 = 2*item4*ICC*(1 - ICC)\n",
    "    #h = item2**2/(item5 + item6 + item7) # corrected degrees of freedom\n",
    "    h = ((N-2)-2*(N_per_cluster-1)*ICC)**2/((N-2)*(1-ICC)**2 + N_per_cluster*(N-2*N_per_cluster)*(ICC**2)+2*(N-2*N_per_cluster)*ICC*(1-ICC))\n",
    "\n",
    "    s=np.sqrt((N*data_exp_pooled.std()**2+N*data_control_pooled.std()**2)/(2*N-2))\n",
    "    #s = sqrt(((N-1)*np.std(data_exp_pooled)**2+(N-1)*np.std(data_control_pooled)**2)/(2*N-2)) # standard deviation of two datasets\n",
    "    t = abs(np.mean(data_exp_pooled) - np.mean(data_control_pooled))/(s*np.sqrt(1/N + 1/N)) # t-test\n",
    "    ta = c*t # corrected t-test\n",
    "    #p_value = 2*sum(tpdf.pdf(np.arange(ta,100,0.001),h)*0.001) # p-value = integral of t-distribution probability function\n",
    "    p_value = 2*(1-tpdf.cdf(ta, h))\n",
    "    #print('P-value based on t-distribution probability function is {:2.2f}'.format(p_value))\n",
    "    return ta, p_value\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def process_data(data_exp, data_control, N_per_cluster, N_clusters, \\\n",
    "                 inter_cluster_SD, intra_cluster_SD, data_method, ttest_method):\n",
    "    \"\"\" This is the function to process data\n",
    "    There are several types of processing\n",
    "    By default it is use simple t-test on pooled data (ignore clustering)\n",
    "    INPUT: 1) the parameters for data generating\n",
    "            2) data_method = {‘pool’, ‘cluster’}, optional\n",
    "               choose the type of data to process furter\n",
    "               ( if 'pool', use the pooled data\n",
    "               elif 'cluster_means' use the means of clusters )\n",
    "            3) ttest_method = {'simple', 'adjusted'}, optional\n",
    "               choose what type of ttest to apply For more information read methods.md\n",
    "     \"\"\"\n",
    "\n",
    "    if data_method == 'pool': # use pooled data for processing\n",
    "        # pool the data into a list:\n",
    "        data_exp_pooled = data_exp.reshape(-1)\n",
    "        data_control_pooled = data_control.reshape(-1)\n",
    "        #print(data_exp, data_control)\n",
    "        if ttest_method == 'simple':\n",
    "            # use simple t-test\n",
    "            t, p_value = ttest(data_exp_pooled, data_control_pooled)\n",
    "        elif ttest_method == 'adjusted': # use adjusted t-test\n",
    "            t, p_value = adj_ttest(N_per_cluster, N_clusters, inter_cluster_SD, \\\n",
    "            intra_cluster_SD, data_exp_pooled, data_control_pooled)\n",
    "        else:\n",
    "            print('insert correct t-test method')\n",
    "    elif data_method == 'cluster':# use means of clusters for processing\n",
    "        data_exp_mean = data_exp.mean(axis=0)\n",
    "        data_control_mean = data_control.mean(axis=0)\n",
    "        if ttest_method == 'simple':\n",
    "            t, p_value = ttest(data_exp_mean, data_control_mean)\n",
    "        elif ttest_method == 'adjusted':\n",
    "            print('can\\'t do adjusted t-test. Need pooled data')\n",
    "            return\n",
    "        else:\n",
    "            print('insert correct t-test method')\n",
    "    return p_value\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def experiment(true_exp_value:float, true_control_value:float, \\\n",
    "               inter_cluster_SD:float, intra_cluster_SD:float, N_clusters:int, \\\n",
    "               N_per_cluster:int, data_method:str = 'pool', \\\n",
    "               ttest_method:str = 'simple'):\n",
    "    \"\"\" This module generates data and asks another module for processing\n",
    "    There are several types of processing\n",
    "    By default it is use simple t-test on pooled data (ignore clustering)\n",
    "    For more information read documentation for process_data\n",
    "    INPUT:  1) the parameters for data generating\n",
    "            2) data_method = {‘pool’, ‘cluster’}, optional\n",
    "            3) ttest_method = {'simple', 'adjusted'}, optional\n",
    "    OUTPUT: the p-value of experiment\n",
    "    EXAMPLE_OF_USE: experiment(1, 1, 0.1, 0.2, 3, 5)\n",
    "                    experiment(1, 1, 0.1, 0.2, 3, 5, 'cluster', 'adjusted') \"\"\"\n",
    "    # generate 2 matrices of data (control and experiment)\n",
    "    data_exp = generate_data(true_exp_value, inter_cluster_SD, intra_cluster_SD, \\\n",
    "                             N_clusters, N_per_cluster)\n",
    "    data_control = generate_data(true_control_value, inter_cluster_SD, \\\n",
    "                                 intra_cluster_SD, N_clusters, N_per_cluster)\n",
    "    # do the processing\n",
    "    p_value = process_data(data_exp, data_control, N_per_cluster, \\\n",
    "                                N_clusters, inter_cluster_SD, intra_cluster_SD, \\\n",
    "                                data_method, ttest_method)\n",
    "    # visualize data\n",
    "    #if show_figure:\n",
    "        #display_data(data_exp, data_control, N_clusters, N_per_cluster)\n",
    "    return p_value\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def error_probability(NN:int, true_exp_value:float, true_control_value:float, \\\n",
    "                      inter_cluster_SD:float, intra_cluster_SD:float, N_clusters:int, \\\n",
    "                      N_per_cluster:int, data_method:str='pool', \\\n",
    "                      ttest_method:str='simple'):\n",
    "    \"\"\" There are two types of errors: 1) False positive 2) False negative\n",
    "    what are the real values?\n",
    "    1) In case of unequal initial values we obtain error if p_value > 0.05\n",
    "       (this means that we agree on zero hypothesis) -> false positive error\n",
    "    2) If the real values are equal we obtain error if p_value < 0.05\n",
    "       (thus we reject zero hypothesis) -> false negative error\n",
    "    INPUT: NN - the number of experiments to conduct\n",
    "           and other parameters for experiment function\n",
    "    OUTPUT: the probability of error \"\"\"\n",
    "    # sign s will easily help to make < reverse\n",
    "    if true_exp_value == true_control_value: s = 1\n",
    "    else: s = -1\n",
    "    # do NN experiments and see how many times we have an error\n",
    "    N_error = 0\n",
    "    for i in range(NN):\n",
    "        p_value = experiment(true_exp_value, true_control_value, inter_cluster_SD,\\\n",
    "                             intra_cluster_SD, N_clusters, N_per_cluster, \\\n",
    "                             data_method, ttest_method)\n",
    "        if s*p_value < s*0.05 :\n",
    "            N_error += 1\n",
    "    return N_error/NN\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def error_probability_heatmap(MAX_N_clusters:int, MAX_N_per_cluster:int, \\\n",
    "                              NN:int, true_exp_value:float, \\\n",
    "                              true_control_value:float, inter_cluster_SD:float, \\\n",
    "                              intra_cluster_SD:float, data_method:str='pool', \\\n",
    "                              ttest_method:str='simple'):\n",
    "    \"\"\" Heatmap will show the error probability for an experimentator's choise\n",
    "    of number of clusters and number of measurements per cluster\n",
    "    INPUT: MAX_N_clusters - maximum number of clusters (vary from 1 to MAX)\n",
    "           MAX_N_per_cluster - maximum number of measurements per cluster\n",
    "           the parameters needed for error_probability function\n",
    "    OUTPUT: a matrix of probability with axis that correspond to the number\n",
    "            of clusters and the number od measurements per cluster  \"\"\"\n",
    "    CLUSTERS = np.array([i for i in range(2,MAX_N_clusters+1)])\n",
    "    PER_CLUSTER = np.array([i for i in range(2,MAX_N_per_cluster+1)])\n",
    "\n",
    "    probability = np.zeros((MAX_N_clusters-1, MAX_N_per_cluster-1))\n",
    "    for i, n_clusters in enumerate(CLUSTERS):\n",
    "        for j, n_per_cluster in enumerate(PER_CLUSTER):\n",
    "            probability[i, j] = error_probability(NN, true_exp_value, \\\n",
    "            true_control_value, inter_cluster_SD, intra_cluster_SD, n_clusters, \\\n",
    "            n_per_cluster, data_method, ttest_method)\n",
    "    return probability\n",
    "    #display_heatmap(probability, CLUSTERS, PER_CLUSTER)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def error_probability_ICC(NN:int, true_exp_value:float, \\\n",
    "                          true_control_value:float, inter_cluster_SD:float, \\\n",
    "                          intra_cluster_SD:float, N_clusters:int, \\\n",
    "                          N_per_cluster:int, data_method:str='pool', \\\n",
    "                          ttest_method:str='simple'):\n",
    "    \"\"\" Let's calculate the probability of erroneus result in dependence of ICC\n",
    "    For this we make the intra_cluster_SD constant and vary inter_cluster_SD\n",
    "    Then call the function that calculates the probability of error for a\n",
    "    set of parameters\n",
    "    INPUT: all the parameters needed for error_probability counting\n",
    "    OUTPUT: a list of error probability for different ICC & ICC \"\"\"\n",
    "\n",
    "    ICC = np.array([0.0, 0.01, 0.03, 0.07, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, \\\n",
    "                    0.4, 0.45, 0.5])\n",
    "    inter_cluster_SDs = np.sqrt(ICC*(intra_cluster_SD**2)/(1-ICC))\n",
    "\n",
    "    probability = np.zeros((len(ICC)))\n",
    "    for i, icc in enumerate(ICC):\n",
    "        probability[i] = error_probability(NN, true_exp_value, \\\n",
    "                                           true_control_value, inter_cluster_SDs[i],\\\n",
    "                                           intra_cluster_SD, N_clusters, \\\n",
    "                                           N_per_cluster, data_method,\\\n",
    "                                           ttest_method)\n",
    "    return probability, ICC\n",
    "    #display_graph(probability, ICC)\n",
    "    \n",
    "def error_probability_ICC(NN:int, true_exp_value:float, \\\n",
    "                        true_control_value:float,  \\\n",
    "                        intra_cluster_SD:float, N_clusters:int, \\\n",
    "                        N_per_cluster:int, ICC, \\\n",
    "                        data_method:str='pool', ttest_method:str='simple'):\n",
    "    \"\"\" Let's calculate the probability of erroneus result in dependence of ICC\n",
    "    For this we make the intra_cluster_SD constant and vary inter_cluster_SD\n",
    "    Then call the function that calculates the probability of error for a\n",
    "    set of parameters\n",
    "\n",
    "    INPUT: all the parameters needed for error_probability counting\n",
    "\n",
    "    OUTPUT: a list of error probability for different ICC & ICC \"\"\"\n",
    "\n",
    "    inter_cluster_SDs = np.sqrt(ICC*(intra_cluster_SD**2)/(1-ICC))\n",
    "\n",
    "    probability = np.zeros((len(ICC)))\n",
    "    for i in range(len(ICC)):\n",
    "        probability[i] = error_probability(NN, true_exp_value, \\\n",
    "                                            true_control_value, inter_cluster_SDs[i],\\\n",
    "                                            intra_cluster_SD, N_clusters, \\\n",
    "                                            N_per_cluster, data_method,\\\n",
    "                                            ttest_method)\n",
    "    return probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns #; sns.set_theme()\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "def display_data(data_exp, data_control, N_clusters:int, N_per_cluster:int):\n",
    "    \"\"\" display data (all experiments and means per clusters)\n",
    "    INPUT: experimental data (matrix) & control data (matrix)\n",
    "           the number of clusters and the number of experiments per clusters\n",
    "    OUTPUT: None \"\"\"\n",
    "    #ipdb.set_trace()\n",
    "    data_exp_mean = data_exp.mean(axis=0)\n",
    "    data_control_mean = data_control.mean(axis=0)\n",
    "    plt.rcParams[\"axes.edgecolor\"] = \"black\"\n",
    "    plt.rcParams[\"axes.linewidth\"] = 1\n",
    "    fig, ax = plt.subplots()\n",
    "    fig = plt.plot(np.ones((N_per_cluster,1))+0.05/np.sqrt(N_clusters)*np.random.randn(N_per_cluster,1), data_exp,'.',markersize=6)\n",
    "\n",
    "    colord = []\n",
    "    for i in range(len(fig)):\n",
    "        colord.append(fig[i].get_color())\n",
    "    col=colord\n",
    "    \n",
    "\n",
    "    plt.scatter(np.ones(N_clusters), data_exp_mean, 1000, col,'+',lineWidths=3)\n",
    "\n",
    "    arr_control=2*np.ones((N_per_cluster,1))+0.05/np.sqrt(N_clusters)*np.random.randn(N_per_cluster,1)\n",
    "    for i in range(N_per_cluster):\n",
    "        for j in range(N_clusters):\n",
    "            plt.plot(arr_control[i], data_control[i][j],'.',markersize=6,color=col[j])\n",
    "    plt.scatter(2*np.ones(N_clusters), data_control_mean, 1000, col,'+',lineWidths=3)\n",
    "\n",
    "    ax.set_xlim(0,3)\n",
    "    ax.patch.set_visible(False)\n",
    "    plt.xticks([])\n",
    "    plt.xlabel('exp                      control') \n",
    "\n",
    "    #  Устанавливаем интервал вспомогательных делений:\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(0.2))\n",
    "    ax.yaxis.set_minor_locator(ticker.MultipleLocator(0.1))\n",
    "    ax.patch.set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.xaxis.set_ticks_position('bottom')\n",
    "    ax.yaxis.set_ticks_position('left')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def display_heatmap(probability, MAX_N_clusters, MAX_N_per_cluster, scaleMax=1):\n",
    "    \"\"\" INPUT: probability is a matrix\n",
    "        OUTPUT: heatmap figure \"\"\"\n",
    "    CLUSTERS = np.array([i for i in range(2,MAX_N_clusters+1)])\n",
    "    PER_CLUSTER = np.array([i for i in range(2,MAX_N_per_cluster+1)])\n",
    "    ax = sns.heatmap(probability.T, xticklabels = CLUSTERS, yticklabels = PER_CLUSTER, vmin = 0, vmax = scaleMax)\n",
    "    ax.invert_yaxis()\n",
    "    plt.xlabel('number of clusters')\n",
    "    plt.ylabel('number of measurements')\n",
    "    #plt.title('All')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def display_graph(probability, ICC, label):\n",
    "    plt.rcParams[\"axes.edgecolor\"] = \"black\"\n",
    "    plt.rcParams[\"axes.linewidth\"] = 1\n",
    "    fig, ax = plt.subplots()\n",
    "    for i in range(len(probability[:,1])):\n",
    "        ax.scatter(ICC, probability[i,:], label=label[i])\n",
    "\n",
    "    ax.legend()\n",
    "    plt.xlabel('ICC')\n",
    "    #  Устанавливаем интервал основных делений:\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(0.1))\n",
    "    #  Устанавливаем интервал вспомогательных делений:\n",
    "    ax.xaxis.set_minor_locator(ticker.MultipleLocator(0.05))\n",
    "\n",
    "    #  Тоже самое проделываем с делениями на оси \"y\":\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(0.1))\n",
    "    ax.yaxis.set_minor_locator(ticker.MultipleLocator(0.05))\n",
    "    ax.patch.set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.xaxis.set_ticks_position('bottom')\n",
    "    ax.yaxis.set_ticks_position('left')\n",
    "    plt.xlabel('ICC')\n",
    "    plt.ylabel('Probability of error')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
